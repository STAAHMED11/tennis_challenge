{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52d72685-9b02-45e2-9743-5108799683cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "import joblib\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal import find_peaks\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "455defa6-06ed-4e97-8fe3-76b55cd55669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 1. FEATURE ENGINEERING FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_optimized_features(df):\n",
    "    \"\"\"\n",
    "    Calculate optimized features for hit/bounce detection.\n",
    "    MUST be identical to training feature engineering.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # === BASIC CLEANING ===\n",
    "    df['x'] = df['x'].replace(0, np.nan).interpolate(method='cubic').bfill().ffill()\n",
    "    df['y'] = df['y'].replace(0, np.nan).interpolate(method='cubic').bfill().ffill()\n",
    "\n",
    "    # === SMOOTHING ===\n",
    "    df['x_smooth'] = gaussian_filter1d(df['x'], sigma=1.5)\n",
    "    df['y_smooth'] = gaussian_filter1d(df['y'], sigma=1.5)\n",
    "\n",
    "    # === 1ST ORDER: VELOCITY ===\n",
    "    df['vx'] = df['x_smooth'].diff()\n",
    "    df['vy'] = df['y_smooth'].diff()\n",
    "    df['speed'] = np.sqrt(df['vx']**2 + df['vy']**2)\n",
    "    df['v_horizontal'] = df['vx'].abs()\n",
    "    df['v_vertical'] = df['vy'].abs()\n",
    "\n",
    "    # === 2ND ORDER: ACCELERATION ===\n",
    "    df['ax'] = df['vx'].diff()\n",
    "    df['ay'] = df['vy'].diff()\n",
    "    df['accel_magnitude'] = np.sqrt(df['ax']**2 + df['ay']**2)\n",
    "    df['accel_horizontal'] = df['ax'].abs()\n",
    "    df['accel_vertical'] = df['ay'].abs()\n",
    "\n",
    "    # === 3RD ORDER: JERK ===\n",
    "    df['jerk_x'] = df['ax'].diff()\n",
    "    df['jerk_y'] = df['ay'].diff()\n",
    "    df['jerk_magnitude'] = np.sqrt(df['jerk_x']**2 + df['jerk_y']**2)\n",
    "\n",
    "    # === ANGLE & DIRECTION ===\n",
    "    df['angle'] = np.arctan2(df['vy'], df['vx'])\n",
    "    df['delta_angle'] = df['angle'].diff().abs()\n",
    "    df.loc[df['delta_angle'] > np.pi, 'delta_angle'] = \\\n",
    "        2*np.pi - df.loc[df['delta_angle'] > np.pi, 'delta_angle']\n",
    "    df['angular_velocity'] = df['delta_angle'] / (df['speed'] + 1e-6)\n",
    "\n",
    "    # === VERTICAL VELOCITY ANALYSIS ===\n",
    "    df['vy_change'] = df['vy'].diff()\n",
    "    df['vy_abs_change'] = df['vy_change'].abs()\n",
    "    df['vy_sign_change'] = ((df['vy'] * df['vy'].shift(1)) < 0).astype(int)\n",
    "    df['vy_acceleration'] = df['vy'].diff(2)\n",
    "\n",
    "    # === HEIGHT & POSITION ===\n",
    "    max_y = df['y'].max()\n",
    "    min_y = df['y'].min()\n",
    "    y_range = max_y - min_y if (max_y - min_y) > 0 else 1\n",
    "\n",
    "    df['height_normalized'] = (max_y - df['y']) / y_range\n",
    "    df['height_raw'] = max_y - df['y']\n",
    "\n",
    "    center_x = (df['x'].max() + df['x'].min()) / 2\n",
    "    df['dist_from_center'] = (df['x'] - center_x).abs()\n",
    "\n",
    "    # === ENERGY & MOMENTUM ===\n",
    "    df['kinetic_energy'] = df['speed']**2\n",
    "    df['energy_change'] = df['kinetic_energy'].diff()\n",
    "    df['energy_change_rate'] = df['energy_change'] / (df['kinetic_energy'].shift(1) + 1e-6)\n",
    "\n",
    "    # === CURVATURE ===\n",
    "    df['curvature'] = np.abs(df['ax'] * df['vy'] - df['ay'] * df['vx']) / \\\n",
    "                      (df['speed']**3 + 1e-6)\n",
    "\n",
    "    # === TEMPORAL CONTEXT ===\n",
    "    temporal_features = ['speed', 'vy', 'accel_magnitude', 'jerk_magnitude',\n",
    "                        'height_normalized', 'delta_angle']\n",
    "\n",
    "    for lag in [1, 2, 3, 5]:\n",
    "        for feature in temporal_features:\n",
    "            if feature in df.columns:\n",
    "                df[f'{feature}_past_{lag}'] = df[feature].shift(lag)\n",
    "                df[f'{feature}_future_{lag}'] = df[feature].shift(-lag)\n",
    "                df[f'{feature}_diff_{lag}'] = df[feature] - df[feature].shift(lag)\n",
    "\n",
    "    for window in [3, 5]:\n",
    "        for feature in temporal_features:\n",
    "            if feature in df.columns:\n",
    "                df[f'{feature}_mean_{window}'] = df[feature].rolling(window, center=True).mean()\n",
    "                df[f'{feature}_std_{window}'] = df[feature].rolling(window, center=True).std()\n",
    "                df[f'{feature}_max_{window}'] = df[feature].rolling(window, center=True).max()\n",
    "                df[f'{feature}_min_{window}'] = df[feature].rolling(window, center=True).min()\n",
    "\n",
    "    df['speed_change'] = df['speed'].diff()\n",
    "    df['accel_change'] = df['accel_magnitude'].diff()\n",
    "    df['height_change'] = df['height_normalized'].diff()\n",
    "\n",
    "    # === PEAK DETECTION ===\n",
    "    df['is_speed_peak'] = 0\n",
    "    df['is_accel_peak'] = 0\n",
    "    df['is_jerk_peak'] = 0\n",
    "    df['is_y_local_max'] = 0\n",
    "    df['is_y_local_min'] = 0\n",
    "\n",
    "    if len(df) > 5:\n",
    "        try:\n",
    "            speed_peaks, _ = find_peaks(df['speed'].values, distance=2, prominence=df['speed'].std()*0.5)\n",
    "            accel_peaks, _ = find_peaks(df['accel_magnitude'].values, distance=2)\n",
    "            jerk_peaks, _ = find_peaks(df['jerk_magnitude'].values, distance=2)\n",
    "            y_maxs, _ = find_peaks(df['y_smooth'].values, distance=3)\n",
    "            y_mins, _ = find_peaks(-df['y_smooth'].values, distance=3)\n",
    "\n",
    "            if len(speed_peaks) > 0:\n",
    "                df.iloc[speed_peaks, df.columns.get_loc('is_speed_peak')] = 1\n",
    "            if len(accel_peaks) > 0:\n",
    "                df.iloc[accel_peaks, df.columns.get_loc('is_accel_peak')] = 1\n",
    "            if len(jerk_peaks) > 0:\n",
    "                df.iloc[jerk_peaks, df.columns.get_loc('is_jerk_peak')] = 1\n",
    "            if len(y_maxs) > 0:\n",
    "                df.iloc[y_maxs, df.columns.get_loc('is_y_local_max')] = 1\n",
    "            if len(y_mins) > 0:\n",
    "                df.iloc[y_mins, df.columns.get_loc('is_y_local_min')] = 1\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # === COMPOSITE FEATURES ===\n",
    "    df['hit_score'] = (\n",
    "        df['jerk_magnitude'] *\n",
    "        df['accel_magnitude'] *\n",
    "        df['delta_angle'] *\n",
    "        (1 + df['is_jerk_peak'])\n",
    "    )\n",
    "\n",
    "    df['bounce_score'] = (\n",
    "        df['vy_abs_change'] *\n",
    "        (1 - df['height_normalized']) *\n",
    "        df['vy_sign_change'] *\n",
    "        (1 + df['is_y_local_min'])\n",
    "    )\n",
    "\n",
    "    df['vertical_impact'] = df['vy_abs_change'] * df['accel_vertical']\n",
    "\n",
    "    # === VELOCITY REVERSAL ===\n",
    "    df['vy_reversed_recently'] = (\n",
    "        (df['vy'] * df['vy_past_1'] < 0) |\n",
    "        (df['vy_past_1'] * df['vy_past_2'] < 0)\n",
    "    ).astype(int)\n",
    "\n",
    "    df['vy_will_reverse'] = (\n",
    "        (df['vy'] * df['vy_future_1'] < 0) |\n",
    "        (df['vy_future_1'] * df['vy_future_2'] < 0)\n",
    "    ).astype(int)\n",
    "\n",
    "    df['vy_reversal_window'] = (\n",
    "        df['vy_reversed_recently'] |\n",
    "        df['vy_sign_change'] |\n",
    "        df['vy_will_reverse']\n",
    "    ).astype(int)\n",
    "\n",
    "    # === SUSTAINED EVENTS ===\n",
    "    df['jerk_sustained'] = (\n",
    "        df['jerk_magnitude_past_3'] +\n",
    "        df['jerk_magnitude'] +\n",
    "        df['jerk_magnitude_future_3']\n",
    "    ) / 3\n",
    "\n",
    "    df['accel_sustained'] = (\n",
    "        df['accel_magnitude_past_3'] +\n",
    "        df['accel_magnitude'] +\n",
    "        df['accel_magnitude_future_3']\n",
    "    ) / 3\n",
    "\n",
    "    # === COMPARATIVE ===\n",
    "    for feature in ['speed', 'vy', 'height_normalized']:\n",
    "        if feature in df.columns:\n",
    "            past_mean = df[f'{feature}_mean_5']\n",
    "            future_mean = df[feature].shift(-5).rolling(5).mean()\n",
    "\n",
    "            df[f'{feature}_past_vs_future'] = past_mean - future_mean\n",
    "\n",
    "            df[f'{feature}_is_peak'] = (\n",
    "                (df[feature] > past_mean) &\n",
    "                (df[feature] > future_mean)\n",
    "            ).astype(int)\n",
    "\n",
    "    df['trajectory_phase'] = np.arctan2(df['vy'], df['vx'] + 1e-6)\n",
    "    df['trajectory_phase_change'] = df['trajectory_phase'].diff().abs()\n",
    "\n",
    "    df['frame_idx'] = range(len(df))\n",
    "    df['time_in_point'] = df['frame_idx'] / len(df)\n",
    "\n",
    "    return df.fillna(0)\n",
    "\n",
    "# ============================================================================\n",
    "# Prepare Dataset Function\n",
    "# ============================================================================\n",
    "def prepare_dataset(folder_path):\n",
    "    \"\"\"\n",
    "    Load all points and prepare dataset with optimized features.\n",
    "    \"\"\"\n",
    "    json_files = [f for f in os.listdir(folder_path) if f.endswith('.json')]\n",
    "    all_dfs = []\n",
    "\n",
    "    print(f\"Loading {len(json_files)} points...\")\n",
    "\n",
    "    for i, filename in enumerate(json_files):\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f\"  Progress: {i+1}/{len(json_files)}\")\n",
    "\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        with open(file_path, 'r') as f:\n",
    "            point_data = json.load(f)\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        frames = sorted(point_data.keys(), key=int)\n",
    "        rows = []\n",
    "        for f_idx in frames:\n",
    "            details = point_data[f_idx]\n",
    "            rows.append({\n",
    "                \"point_id\": filename.replace(\".json\", \"\"),\n",
    "                \"frame\": int(f_idx),\n",
    "                \"x\": details.get(\"x\"),\n",
    "                \"y\": details.get(\"y\"),\n",
    "                \"visible\": details.get(\"visible\"),\n",
    "                \"action\": details.get(\"action\")\n",
    "            })\n",
    "\n",
    "        df_point = pd.DataFrame(rows)\n",
    "        df_point = calculate_optimized_features(df_point)\n",
    "        all_dfs.append(df_point)\n",
    "\n",
    "    # Combine all\n",
    "    full_df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Dataset created: {len(full_df)} frames\")\n",
    "    print(f\"Features: {len(full_df.columns)}\")\n",
    "    print(f\"Class distribution:\\n{full_df['action'].value_counts()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    return full_df\n",
    "\n",
    "# ============================================================================\n",
    "# Prepare Balanced Dataset Function\n",
    "# ============================================================================\n",
    "def prepare_balanced_dataset(df, strategy='hybrid'):\n",
    "\n",
    "\n",
    "    df_hit = df[df['action'] == 'hit'].copy()\n",
    "    df_bounce = df[df['action'] == 'bounce'].copy()\n",
    "    df_air = df[df['action'] == 'air'].copy()\n",
    "\n",
    "    n_hit = len(df_hit)\n",
    "    n_bounce = len(df_bounce)\n",
    "    n_minority = n_hit + n_bounce\n",
    "    target_air_ratio = 10\n",
    "    target_air_samples = target_air_ratio * n_minority\n",
    "\n",
    "    print(f\"\\nHybrid Strategy:\")\n",
    "    print(f\"  Keeping all hits: {n_hit}\")\n",
    "    print(f\"  Keeping all bounces: {n_bounce}\")\n",
    "    print(f\"  Downsampling air: {len(df_air)} → {target_air_samples}\")\n",
    "    print(f\"  Target ratio - Air:Hit:Bounce = {target_air_ratio}:1:1\")\n",
    "\n",
    "    # Downsample air\n",
    "    df_air_downsampled = df_air.sample(n=target_air_samples, random_state=42)\n",
    "\n",
    "    # Combine\n",
    "    df_balanced = pd.concat([df_hit, df_bounce, df_air_downsampled], ignore_index=True)\n",
    "    df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # Final distribution\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"Balanced class distribution:\")\n",
    "    print(df_balanced['action'].value_counts())\n",
    "    print(f\"Total samples: {len(df_balanced)}\")\n",
    "    print(f\"Reduction: {len(df)} → {len(df_balanced)} ({100*len(df_balanced)/len(df):.1f}%)\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    return df_balanced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01f57e2c-e005-4408-86aa-f205d8d081f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 313 points...\n",
      "  Progress: 50/313\n",
      "  Progress: 100/313\n",
      "  Progress: 150/313\n",
      "  Progress: 200/313\n",
      "  Progress: 250/313\n",
      "  Progress: 300/313\n",
      "\n",
      "============================================================\n",
      "Dataset created: 177341 frames\n",
      "Features: 181\n",
      "Class distribution:\n",
      "action\n",
      "air       174295\n",
      "hit         1600\n",
      "bounce      1446\n",
      "Name: count, dtype: int64\n",
      "============================================================\n",
      "\n",
      "Hybrid Strategy:\n",
      "  Keeping all hits: 1600\n",
      "  Keeping all bounces: 1446\n",
      "  Downsampling air: 174295 → 30460\n",
      "  Target ratio - Air:Hit:Bounce = 10:1:1\n",
      "\n",
      "======================================================================\n",
      "Balanced class distribution:\n",
      "action\n",
      "air       30460\n",
      "hit        1600\n",
      "bounce     1446\n",
      "Name: count, dtype: int64\n",
      "Total samples: 33506\n",
      "Reduction: 177341 → 33506 (18.9%)\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare training data\n",
    "folder_path = \"per_point_v2\"\n",
    "\n",
    "df_train = prepare_dataset(folder_path)\n",
    "df_balanced = prepare_balanced_dataset(df_train, strategy='hybrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0894fe04-2945-48be-93a5-23d81ea7f529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Calculate Class Weights Function\n",
    "# ============================================================================\n",
    "def calculate_class_weights(y_train):\n",
    "    \"\"\"\n",
    "    Calculate class weights for model training.\n",
    "    Use this with any strategy.\n",
    "    \"\"\"\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y_train)\n",
    "\n",
    "    classes = np.unique(y_encoded)\n",
    "    weights = compute_class_weight('balanced', classes=classes, y=y_encoded)\n",
    "\n",
    "    weight_dict = dict(zip(le.classes_, weights))\n",
    "\n",
    "    print(\"\\nClass weights:\")\n",
    "    for cls, weight in weight_dict.items():\n",
    "        print(f\"  {cls:8s}: {weight:.4f}\")\n",
    "\n",
    "    return weight_dict, le\n",
    "\n",
    "# ============================================================================\n",
    "# Train Traditional ML Model Function\n",
    "# ============================================================================\n",
    "def train_traditional_ml(X_train, y_train, X_test, y_test, model_type='xgboost'):\n",
    "    \"\"\"\n",
    "    Train traditional ML models with optimized hyperparameters.\n",
    "    Enhanced for imbalanced multi-class hit/bounce detection.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Training {model_type.upper()}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    # Encode labels\n",
    "    le = LabelEncoder()\n",
    "    y_train_encoded = le.fit_transform(y_train)\n",
    "    y_test_encoded = le.transform(y_test)\n",
    "\n",
    "    # Calculate class weights for imbalanced data\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "    class_weights = compute_class_weight('balanced',\n",
    "                                         classes=np.unique(y_train_encoded),\n",
    "                                         y=y_train_encoded)\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "    print(f\"Class distribution:\")\n",
    "    for cls, count in zip(le.classes_, np.bincount(y_train_encoded)):\n",
    "        print(f\"  {cls}: {count} samples (weight: {class_weight_dict[le.transform([cls])[0]]:.3f})\")\n",
    "\n",
    "    if model_type == 'xgboost':\n",
    "        import xgboost as xgb\n",
    "\n",
    "        # Enhanced XGBoost parameters\n",
    "        model = xgb.XGBClassifier(\n",
    "            # Core parameters\n",
    "            n_estimators=500,\n",
    "            max_depth=10,\n",
    "            learning_rate=0.05,\n",
    "\n",
    "            # Regularization - prevent overfitting\n",
    "            min_child_weight=3,\n",
    "            gamma=0.1,\n",
    "            reg_alpha=0.1,\n",
    "            reg_lambda=1.0,\n",
    "\n",
    "            # Sampling parameters\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            colsample_bylevel=0.8,\n",
    "            colsample_bynode=0.8,\n",
    "\n",
    "            # Class imbalance handling\n",
    "            scale_pos_weight=class_weights[1]/class_weights[0] if len(class_weights) == 2 else 1,\n",
    "\n",
    "            # Performance\n",
    "            tree_method='hist',\n",
    "            max_bin=256,\n",
    "\n",
    "            # Training control\n",
    "            early_stopping_rounds=50,\n",
    "            eval_metric='mlogloss',\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "        # Use validation set for early stopping\n",
    "        eval_set = [(X_test, y_test_encoded)]\n",
    "        model.fit(X_train, y_train_encoded,\n",
    "                 eval_set=eval_set,\n",
    "                 verbose=False)\n",
    "\n",
    "    elif model_type == 'lightgbm':\n",
    "        import lightgbm as lgb\n",
    "\n",
    "        # Enhanced LightGBM parameters\n",
    "        model = lgb.LGBMClassifier(\n",
    "            # Core parameters\n",
    "            n_estimators=2500,\n",
    "            max_depth=16,\n",
    "            learning_rate=0.05,\n",
    "            num_leaves=80,\n",
    "\n",
    "            # Regularization\n",
    "            min_child_samples=20,\n",
    "            min_child_weight=0.001,\n",
    "            reg_alpha=0.1,\n",
    "            reg_lambda=1.0,\n",
    "\n",
    "            # Sampling\n",
    "            subsample=0.8,\n",
    "            subsample_freq=5,\n",
    "            colsample_bytree=0.8,\n",
    "\n",
    "            # Feature interaction\n",
    "            feature_fraction_bynode=0.8,\n",
    "\n",
    "            # Class imbalance\n",
    "            class_weight='balanced',\n",
    "            is_unbalance=True,\n",
    "\n",
    "            # Performance\n",
    "            boosting_type='gbdt',\n",
    "            n_jobs=-1,\n",
    "\n",
    "            # Training control\n",
    "            early_stopping_rounds=50,\n",
    "            random_state=42,\n",
    "            verbose=-1\n",
    "        )\n",
    "\n",
    "        eval_set = [(X_test, y_test_encoded)]\n",
    "        model.fit(X_train, y_train_encoded,\n",
    "                 eval_set=eval_set,\n",
    "                 callbacks=[lgb.early_stopping(50, verbose=False)])\n",
    "\n",
    "    elif model_type == 'catboost':\n",
    "        from catboost import CatBoostClassifier\n",
    "\n",
    "        # Enhanced CatBoost parameters\n",
    "        model = CatBoostClassifier(\n",
    "            # Core parameters\n",
    "            iterations=500,\n",
    "            depth=10,                      # Increased depth\n",
    "            learning_rate=0.05,\n",
    "\n",
    "            # Regularization\n",
    "            l2_leaf_reg=3.0,              # NEW: L2 regularization\n",
    "            bagging_temperature=1.0,       # NEW: Bayesian bootstrap intensity\n",
    "\n",
    "            # Sampling\n",
    "            # subsample=0.8, # Removed as it's incompatible with Bayesian bootstrap\n",
    "\n",
    "            # Class imbalance\n",
    "            auto_class_weights='Balanced', # NEW: automatic balancing\n",
    "            # class_weights=class_weight_dict, # Removed due to conflict with auto_class_weights\n",
    "\n",
    "            # CatBoost specific\n",
    "            border_count=128,              # NEW: splits for numerical features\n",
    "            grow_policy='SymmetricTree',   # Default, but explicit\n",
    "            bootstrap_type='Bayesian',     # NEW: Bayesian bootstrap\n",
    "\n",
    "            # Training control\n",
    "            early_stopping_rounds=50,\n",
    "            use_best_model=True,\n",
    "\n",
    "            # Performance\n",
    "            task_type='CPU',\n",
    "            thread_count=-1,\n",
    "            random_state=42,\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        eval_set = (X_test, y_test_encoded)\n",
    "        model.fit(X_train, y_train_encoded,\n",
    "                 eval_set=eval_set,\n",
    "                 verbose=False)\n",
    "\n",
    "    elif model_type == 'random_forest':\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "        # Enhanced Random Forest parameters\n",
    "        model = RandomForestClassifier(\n",
    "            # Core parameters\n",
    "            n_estimators=500,              # Increased from 200\n",
    "            max_depth=25,                  # Increased from 20\n",
    "\n",
    "            # Split parameters\n",
    "            min_samples_split=5,           # Reduced from 10\n",
    "            min_samples_leaf=2,            # Reduced from 4\n",
    "            max_features='sqrt',           # NEW: sqrt(n_features) per split\n",
    "\n",
    "            # Regularization\n",
    "            min_weight_fraction_leaf=0.0,\n",
    "            max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0,\n",
    "\n",
    "            # Sampling\n",
    "            bootstrap=True,\n",
    "            oob_score=True,                # NEW: out-of-bag score\n",
    "\n",
    "            # Class imbalance\n",
    "            class_weight='balanced_subsample', # NEW: balanced per bootstrap\n",
    "\n",
    "            # Performance\n",
    "            n_jobs=-1,\n",
    "            random_state=42,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        model.fit(X_train, y_train_encoded)\n",
    "\n",
    "        if hasattr(model, 'oob_score_'):\n",
    "            print(f\"\\nOut-of-Bag Score: {model.oob_score_:.4f}\")\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # EVALUATION\n",
    "    # ========================================================================\n",
    "\n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Probabilities (if available)\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_pred_proba = model.predict_proba(X_test)\n",
    "        print(f\"\\nPrediction confidence (mean): {y_pred_proba.max(axis=1).mean():.4f}\")\n",
    "\n",
    "    # Classification report\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TEST SET PERFORMANCE\")\n",
    "    print(\"=\"*70)\n",
    "    print(classification_report(y_test_encoded, y_pred,\n",
    "                                zero_division=0,\n",
    "                                target_names=le.classes_,\n",
    "                                digits=4))\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test_encoded, y_pred, labels=np.arange(len(le.classes_)))\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    class_names = le.classes_\n",
    "    header = \"            \" + \"  \".join([f\"{c:>8s}\" for c in class_names])\n",
    "    print(header)\n",
    "    for i, label in enumerate(class_names):\n",
    "        row_str = f\"{label:12s}\"\n",
    "        for val in cm[i]:\n",
    "            row_str += f\"{val:8d}  \"\n",
    "        print(row_str)\n",
    "\n",
    "    # Per-class metrics\n",
    "    print(\"\\nPer-Class Metrics:\")\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "    precision, recall, f1_per_class, support = precision_recall_fscore_support(\n",
    "        y_test_encoded, y_pred, zero_division=0\n",
    "    )\n",
    "\n",
    "    for i, cls in enumerate(le.classes_):\n",
    "        print(f\"  {cls:8s}: Precision={precision[i]:.4f}, Recall={recall[i]:.4f}, \"\n",
    "              f\"F1={f1_per_class[i]:.4f}, Support={support[i]}\")\n",
    "\n",
    "    # Overall metrics\n",
    "    f1_macro = f1_score(y_test_encoded, y_pred, average='macro', zero_division=0)\n",
    "    f1_weighted = f1_score(y_test_encoded, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "    print(f\"\\nOverall Metrics:\")\n",
    "    print(f\"  Macro F1:    {f1_macro:.4f}\")\n",
    "    print(f\"  Weighted F1: {f1_weighted:.4f}\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # FEATURE IMPORTANCE\n",
    "    # ========================================================================\n",
    "\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': X_train.columns,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"TOP 30 MOST IMPORTANT FEATURES\")\n",
    "        print(\"=\"*70)\n",
    "        print(importance_df.head(30).to_string(index=False))\n",
    "\n",
    "        # Save full importance\n",
    "        importance_df.to_csv(f'{model_type}_feature_importance.csv', index=False)\n",
    "        print(f\"\\n✓ Full feature importance saved to '{model_type}_feature_importance.csv'\")\n",
    "\n",
    "    return model, f1_macro, le\n",
    "\n",
    "# ============================================================================\n",
    "# Train and Save Model Function\n",
    "# ============================================================================\n",
    "def train_and_save_model(df_train, model_type='xgboost', output_dir='models/'):\n",
    "    \"\"\"\n",
    "    Train model and save all necessary files for inference.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"TRAINING AND SAVING MODEL\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    # Prepare data\n",
    "    feature_cols = [col for col in df_train.columns\n",
    "                   if col not in ['point_id', 'frame', 'action', 'visible']]\n",
    "\n",
    "    X = df_train[feature_cols].fillna(0)\n",
    "    y = df_train['action']\n",
    "\n",
    "    # Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # Scale\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_cols)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_cols)\n",
    "\n",
    "    # Train\n",
    "    model, f1, le = train_traditional_ml(\n",
    "        X_train_scaled, y_train,\n",
    "        X_test_scaled, y_test,\n",
    "        model_type=model_type\n",
    "    )\n",
    "\n",
    "    # ========================================================================\n",
    "    # SAVE ALL NECESSARY FILES\n",
    "    # ========================================================================\n",
    "\n",
    "    model_path = os.path.join(output_dir, f'trained_{model_type}_model.pkl')\n",
    "    scaler_path = os.path.join(output_dir, 'scaler.pkl')\n",
    "    le_path = os.path.join(output_dir, 'label_encoder.pkl')\n",
    "    feature_path = os.path.join(output_dir, 'feature_columns.pkl')\n",
    "\n",
    "    joblib.dump(model, model_path)\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    joblib.dump(le, le_path)\n",
    "    joblib.dump(feature_cols, feature_path)\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"MODEL SAVED SUCCESSFULLY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Model:          {model_path}\")\n",
    "    print(f\"Scaler:         {scaler_path}\")\n",
    "    print(f\"Label Encoder:  {le_path}\")\n",
    "    print(f\"Feature Cols:   {feature_path}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    return model, scaler, le, feature_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cb16662-5e86-45c6-8fa0-98e8283a1514",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_save_model(df_train, model_type='xgboost', output_dir='models/'):\n",
    "    \"\"\"\n",
    "    Train model and save all necessary files for inference.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"TRAINING AND SAVING MODEL\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    # Prepare data\n",
    "    feature_cols = [col for col in df_train.columns\n",
    "                   if col not in ['point_id', 'frame', 'action', 'visible']]\n",
    "\n",
    "    X = df_train[feature_cols].fillna(0)\n",
    "    y = df_train['action']\n",
    "\n",
    "    # Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # Scale\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_cols)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_cols)\n",
    "\n",
    "    # Train\n",
    "    model, f1, le = train_traditional_ml(\n",
    "        X_train_scaled, y_train,\n",
    "        X_test_scaled, y_test,\n",
    "        model_type=model_type\n",
    "    )\n",
    "\n",
    "    # ========================================================================\n",
    "    # SAVE ALL NECESSARY FILES\n",
    "    # ========================================================================\n",
    "\n",
    "    model_path = os.path.join(output_dir, f'trained_{model_type}_model.pkl')\n",
    "    scaler_path = os.path.join(output_dir, 'scaler.pkl')\n",
    "    le_path = os.path.join(output_dir, 'label_encoder.pkl')\n",
    "    feature_path = os.path.join(output_dir, 'feature_columns.pkl')\n",
    "\n",
    "    joblib.dump(model, model_path)\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    joblib.dump(le, le_path)\n",
    "    joblib.dump(feature_cols, feature_path)\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"MODEL SAVED SUCCESSFULLY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Model:          {model_path}\")\n",
    "    print(f\"Scaler:         {scaler_path}\")\n",
    "    print(f\"Label Encoder:  {le_path}\")\n",
    "    print(f\"Feature Cols:   {feature_path}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    return model, scaler, le, feature_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59d81afe-b56b-44d4-91ac-4f6e2a9c4ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in /home/ahmedsta/.local/lib/python3.8/site-packages (4.6.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /home/ahmedsta/.local/lib/python3.8/site-packages (from lightgbm) (1.24.4)\n",
      "Requirement already satisfied: scipy in /home/ahmedsta/.local/lib/python3.8/site-packages (from lightgbm) (1.10.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45967777-31c4-4762-a65a-a06351cd9553",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b304298e-7e3a-4724-9e40-57bfa389c539",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TRAINING AND SAVING MODEL\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Training LIGHTGBM\n",
      "======================================================================\n",
      "Class distribution:\n",
      "  air: 24367 samples (weight: 0.367)\n",
      "  bounce: 1157 samples (weight: 7.722)\n",
      "  hit: 1280 samples (weight: 6.980)\n",
      "\n",
      "Prediction confidence (mean): 0.9796\n",
      "\n",
      "======================================================================\n",
      "TEST SET PERFORMANCE\n",
      "======================================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         air     0.9898    0.9723    0.9810      6093\n",
      "      bounce     0.8055    0.9170    0.8576       289\n",
      "         hit     0.7165    0.8688    0.7853       320\n",
      "\n",
      "    accuracy                         0.9649      6702\n",
      "   macro avg     0.8373    0.9193    0.8746      6702\n",
      "weighted avg     0.9688    0.9649    0.9663      6702\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "                 air    bounce       hit\n",
      "air             5924        59       110  \n",
      "bounce            24       265         0  \n",
      "hit               37         5       278  \n",
      "\n",
      "Per-Class Metrics:\n",
      "  air     : Precision=0.9898, Recall=0.9723, F1=0.9810, Support=6093\n",
      "  bounce  : Precision=0.8055, Recall=0.9170, F1=0.8576, Support=289\n",
      "  hit     : Precision=0.7165, Recall=0.8688, F1=0.7853, Support=320\n",
      "\n",
      "Overall Metrics:\n",
      "  Macro F1:    0.8746\n",
      "  Weighted F1: 0.9663\n",
      "\n",
      "======================================================================\n",
      "TOP 30 MOST IMPORTANT FEATURES\n",
      "======================================================================\n",
      "                 feature  importance\n",
      "                       y         987\n",
      "accel_magnitude_future_3         601\n",
      "height_normalized_diff_1         560\n",
      "        dist_from_center         559\n",
      "           time_in_point         555\n",
      "                y_smooth         541\n",
      " jerk_magnitude_future_5         521\n",
      "accel_magnitude_future_5         517\n",
      "        accel_horizontal         507\n",
      "               frame_idx         500\n",
      " height_normalized_std_3         490\n",
      "    delta_angle_future_3         446\n",
      "                  jerk_x         433\n",
      "    delta_angle_future_2         417\n",
      "   jerk_magnitude_diff_3         416\n",
      "  accel_magnitude_past_3         415\n",
      "    delta_angle_future_5         412\n",
      "    jerk_magnitude_std_3         387\n",
      "   jerk_magnitude_diff_1         373\n",
      "accel_magnitude_future_2         372\n",
      " jerk_magnitude_future_3         368\n",
      "       delta_angle_min_5         362\n",
      "            v_horizontal         359\n",
      "  accel_magnitude_past_5         353\n",
      "      delta_angle_past_5         353\n",
      "       vy_past_vs_future         351\n",
      "                       x         351\n",
      "       delta_angle_min_3         346\n",
      "              height_raw         345\n",
      "   jerk_magnitude_past_3         344\n",
      "\n",
      "✓ Full feature importance saved to 'lightgbm_feature_importance.csv'\n",
      "\n",
      "======================================================================\n",
      "MODEL SAVED SUCCESSFULLY\n",
      "======================================================================\n",
      "Model:          models/trained_lightgbm_model.pkl\n",
      "Scaler:         models/scaler.pkl\n",
      "Label Encoder:  models/label_encoder.pkl\n",
      "Feature Cols:   models/feature_columns.pkl\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and save\n",
    "model, scaler, le, feature_cols = train_and_save_model(\n",
    "    df_balanced,\n",
    "    model_type='lightgbm',\n",
    "    output_dir='models/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "759719ef-ae90-4acf-9940-b63f3ed139fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def remove_action_field(input_json_path, output_json_path):\n",
    "    with open(input_json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for key, value in data.items():\n",
    "        if isinstance(value, dict):\n",
    "            value.pop(\"action\", None)  # safely remove if exists\n",
    "\n",
    "    with open(output_json_path, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "    print(\"✔ 'action' field removed and file saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0841a61f-2a5a-4610-91e6-44aaf9845326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ 'action' field removed and file saved.\n"
     ]
    }
   ],
   "source": [
    "remove_action_field(\n",
    "    \"per_point_v2/ball_data_5.json\",\n",
    "    \"output.json\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
