{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e75cf3a-2ab1-4bf0-8a2b-3736ddf8b571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def supervised_hit_bounce_detection(json_file_path, model_path='trained_model.pkl',\n",
    "                                    scaler_path='scaler.pkl',\n",
    "                                    label_encoder_path='label_encoder.pkl',\n",
    "                                    feature_cols_path='feature_columns.pkl'):\n",
    "    \"\"\"\n",
    "    Supervised hit and bounce detection for a single point.\n",
    "\n",
    "    Args:\n",
    "        json_file_path: Path to ball tracking JSON file\n",
    "        model_path: Path to trained model (.pkl)\n",
    "        scaler_path: Path to fitted scaler (.pkl)\n",
    "        label_encoder_path: Path to label encoder (.pkl)\n",
    "        feature_cols_path: Path to feature column names (.pkl)\n",
    "\n",
    "    Returns:\n",
    "        enriched_data: Dictionary with original data + 'pred_action' for each frame\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"SUPERVISED HIT/BOUNCE DETECTION\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Input file: {json_file_path}\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # STEP 1: Load the JSON data\n",
    "    # ========================================================================\n",
    "\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        ball_data = json.load(f)\n",
    "\n",
    "    print(f\"Loaded {len(ball_data)} frames\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # STEP 2: Convert to DataFrame\n",
    "    # ========================================================================\n",
    "\n",
    "    frames = sorted(ball_data.keys(), key=int)\n",
    "    rows = []\n",
    "    for frame_idx in frames:\n",
    "        details = ball_data[frame_idx]\n",
    "        rows.append({\n",
    "            \"frame\": int(frame_idx),\n",
    "            \"x\": details.get(\"x\"),\n",
    "            \"y\": details.get(\"y\"),\n",
    "            \"visible\": details.get(\"visible\"),\n",
    "            \"action\": details.get(\"action\", \"unknown\")  # May not exist in test data\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    print(f\"DataFrame shape: {df.shape}\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # STEP 3: Feature Engineering\n",
    "    # ========================================================================\n",
    "\n",
    "    print(\"Calculating features...\")\n",
    "    df_features = calculate_optimized_features(df)\n",
    "    print(f\"Features calculated: {len(df_features.columns)} columns\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # STEP 4: Load trained model and preprocessors\n",
    "    # ========================================================================\n",
    "\n",
    "    print(\"Loading trained model...\")\n",
    "    model = joblib.load(model_path)\n",
    "    scaler = joblib.load(scaler_path)\n",
    "    label_encoder = joblib.load(label_encoder_path)\n",
    "    feature_cols = joblib.load(feature_cols_path)\n",
    "\n",
    "    print(f\"Model loaded: {type(model).__name__}\")\n",
    "    print(f\"Expected features: {len(feature_cols)}\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # STEP 5: Prepare features for prediction\n",
    "    # ========================================================================\n",
    "\n",
    "    # Select only the features used during training\n",
    "    X = df_features[feature_cols].fillna(0)\n",
    "\n",
    "    # Scale features\n",
    "    X_scaled = scaler.transform(X)\n",
    "    X_scaled = pd.DataFrame(X_scaled, columns=feature_cols)\n",
    "\n",
    "    print(f\"Feature matrix shape: {X_scaled.shape}\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # STEP 6: Make predictions\n",
    "    # ========================================================================\n",
    "\n",
    "    print(\"Making predictions...\")\n",
    "    y_pred_encoded = model.predict(X_scaled)\n",
    "    y_pred = label_encoder.inverse_transform(y_pred_encoded)\n",
    "\n",
    "    # Get prediction probabilities (if available)\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_pred_proba = model.predict_proba(X_scaled)\n",
    "        confidence = y_pred_proba.max(axis=1)\n",
    "    else:\n",
    "        confidence = np.ones(len(y_pred))\n",
    "\n",
    "    # ========================================================================\n",
    "    # STEP 7: Create enriched JSON\n",
    "    # ========================================================================\n",
    "\n",
    "    enriched_data = {}\n",
    "\n",
    "    for i, frame_idx in enumerate(frames):\n",
    "        enriched_data[frame_idx] = {\n",
    "            \"x\": ball_data[frame_idx].get(\"x\"),\n",
    "            \"y\": ball_data[frame_idx].get(\"y\"),\n",
    "            \"visible\": ball_data[frame_idx].get(\"visible\"),\n",
    "            \"action\": ball_data[frame_idx].get(\"action\", \"unknown\"),\n",
    "            \"pred_action\": y_pred[i],\n",
    "            \"confidence\": float(confidence[i])\n",
    "        }\n",
    "\n",
    "    # ========================================================================\n",
    "    # STEP 8: Summary statistics\n",
    "    # ========================================================================\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"PREDICTION SUMMARY\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    pred_counts = pd.Series(y_pred).value_counts()\n",
    "    print(\"\\nPredicted distribution:\")\n",
    "    for action, count in pred_counts.items():\n",
    "        print(f\"  {action:8s}: {count:5d} frames ({100*count/len(y_pred):.2f}%)\")\n",
    "\n",
    "    print(f\"\\nMean confidence: {confidence.mean():.4f}\")\n",
    "    print(f\"Min confidence:  {confidence.min():.4f}\")\n",
    "    print(f\"Max confidence:  {confidence.max():.4f}\")\n",
    "\n",
    "    # If ground truth exists, calculate accuracy\n",
    "    if \"action\" in ball_data[frames[0]] and ball_data[frames[0]][\"action\"] != \"unknown\":\n",
    "        y_true = [ball_data[f][\"action\"] for f in frames]\n",
    "        accuracy = np.mean([yt == yp for yt, yp in zip(y_true, y_pred)])\n",
    "        print(f\"\\nAccuracy (vs ground truth): {accuracy:.4f}\")\n",
    "\n",
    "        from sklearn.metrics import classification_report\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_true, y_pred, zero_division=0))\n",
    "\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    return enriched_data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
